{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Лабораторная работа №3: Решающие деревья\n","\t1.\tПодготовка данных:\n","\t•\tАналогично предыдущим работам: обработка категориальных переменных, нормализация, разделение на обучающие и тестовые выборки.\n","\t2.\tБазовые модели:\n","\t•\tОбучение DecisionTreeClassifier и DecisionTreeRegressor.\n","\t•\tОценка качества по метрикам accuracy, MSE, R².\n","\t3.\tУлучшение бейзлайна:\n","\t•\tПодбор гиперпараметров (max_depth, min_samples_split, min_samples_leaf) с помощью GridSearchCV или RandomizedSearchCV.\n","\t4.\tПользовательская реализация:\n","\t•\tРеализация кастомных алгоритмов для решающих деревьев (классификация и регрессия).\n","\t•\tОценка их качества и сравнение с библиотечными реализациями."],"metadata":{"id":"FVJAPqJxyhn1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tawKmlRrqHf7"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n","from sklearn.metrics import accuracy_score, mean_squared_error\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Загрузка данных для классификации\n","# Чтение CSV файла, содержащего текстовые данные (job_title) и целевые категории (category).\n","classification_data = pd.read_csv(\"/content/drive/MyDrive/AIMAI/ds1.csv\")\n","\n","# Предварительная обработка данных\n","# Удаление строк с пропущенными значениями, чтобы избежать ошибок при обработке текста.\n","classification_data = classification_data.dropna()\n","\n","# Выделение признаков и целевой переменной\n","# job_title: текстовые данные, которые будут преобразованы в числовую форму.\n","# category: целевая переменная, представляющая классы.\n","X_text = classification_data['job_title']  # Признаки (текстовые данные)\n","y_class = classification_data['category']  # Целевая переменная\n","\n","# Преобразование текстовых данных в числовые\n","# Используем метод Bag of Words (CountVectorizer), чтобы представить текстовые данные в виде числовой матрицы.\n","# Каждая строка текста преобразуется в вектор, где значения представляют количество вхождений слов.\n","vectorizer = CountVectorizer()\n","X_class = vectorizer.fit_transform(X_text)\n","\n","# Разделение данных на обучающую и тестовую выборки\n","# Разделяем данные на:\n","# - Обучающую выборку (80%): используется для обучения модели.\n","# - Тестовую выборку (20%): используется для проверки качества модели.\n","# random_state=42 фиксирует случайное состояние для воспроизводимости.\n","X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n","    X_class, y_class, test_size=0.2, random_state=42\n",")"]},{"cell_type":"code","source":["# Загрузка данных для задачи регрессии\n","# Читаем CSV файл с признаками и целевой переменной.\n","regression_data = pd.read_csv(\"/content/drive/MyDrive/AIMAI/ds2.csv\")\n","\n","# Удаление ненужных столбцов\n","# Исключаем:\n","# - \"price\" (целевую переменную, которую будем предсказывать),\n","# - \"Address\" и \"desc\" (текстовые столбцы, не участвующие в модели).\n","X_reg = regression_data.drop(columns=[\"price\", \"Address\", \"desc\"])\n","y_reg = regression_data[\"price\"]  # Выделяем целевую переменную.\n","\n","# Преобразование категориальных данных в числовые\n","# Преобразуем текстовые и категориальные признаки в числовые с помощью метода one-hot encoding.\n","# drop_first=True удаляет первый уровень категории, чтобы избежать мультиколлинеарности.\n","X_reg = pd.get_dummies(X_reg, drop_first=True)\n","\n","# Разделение данных на обучающую и тестовую выборки\n","# Тренировочная выборка (80%): для обучения модели.\n","# Тестовая выборка (20%): для проверки качества модели.\n","# random_state=42 фиксирует случайное состояние для воспроизводимости результатов.\n","X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n","    X_reg, y_reg, test_size=0.2, random_state=42\n",")"],"metadata":{"id":"nbjQgj2Mvkc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Создание экземпляра решающего дерева для задачи классификации\n","# random_state=42 фиксирует случайное состояние для обеспечения воспроизводимости результата.\n","dt_classifier = DecisionTreeClassifier(random_state=42)\n","\n","# Обучение модели решающего дерева\n","# X_train_class: обучающие признаки (матрица признаков).\n","# y_train_class: обучающие метки классов.\n","dt_classifier.fit(X_train_class, y_train_class)\n","\n","# Оценка качества модели\n","# Предсказание классов для тестовой выборки.\n","y_pred_class = dt_classifier.predict(X_test_class)\n","\n","# Вычисление точности (accuracy)\n","# Сравниваем предсказанные метки (y_pred_class) с реальными метками (y_test_class).\n","accuracy = accuracy_score(y_test_class, y_pred_class)\n","\n","# Вывод точности модели\n","print(f'Accuracy for classification: {accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"niCNykcEvn1M","outputId":"e5a4991b-4643-4123-a5db-abb464bace4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy for classification: 0.7923\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Создание экземпляра решающего дерева для задачи регрессии\n","# random_state=42 фиксирует случайное состояние для воспроизводимости результата.\n","dt_regressor = DecisionTreeRegressor(random_state=42)\n","\n","# Обучение модели решающего дерева\n","# X_train_reg: обучающие признаки (матрица признаков).\n","# y_train_reg: целевая переменная для обучающей выборки.\n","dt_regressor.fit(X_train_reg, y_train_reg)\n","\n","# Прогнозирование значений целевой переменной на тестовой выборке\n","# X_test_reg: тестовые признаки (матрица признаков).\n","y_pred_reg = dt_regressor.predict(X_test_reg)\n","\n","# Оценка качества модели\n","\n","# MAE (Mean Absolute Error): средняя абсолютная ошибка.\n","# Показывает среднее абсолютное отклонение предсказанных значений от реальных.\n","mae = mean_absolute_error(y_test_reg, y_pred_reg)\n","\n","# MSE (Mean Squared Error): среднеквадратичная ошибка.\n","# Указывает на средний квадрат разницы между предсказанными и реальными значениями.\n","mse = mean_squared_error(y_test_reg, y_pred_reg)\n","\n","# R² (R-squared): коэффициент детерминации.\n","# Показывает, насколько хорошо модель объясняет дисперсию данных (значение от 0 до 1, где 1 — идеально).\n","r2 = r2_score(y_test_reg, y_pred_reg)\n","\n","# Вывод метрик качества модели\n","print(f\"MAE: {mae:.4f}\")  # Средняя абсолютная ошибка\n","print(f\"MSE: {mse:.4f}\")  # Среднеквадратичная ошибка\n","print(f\"R^2: {r2:.4f}\")   # Коэффициент детерминации"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zGu868invqwg","outputId":"9a333ebd-df96-473b-a9aa-6e10fc62015f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MAE: 230981.9121\n","MSE: 822028294573.6434\n","R^2: 0.9861\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n","from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.impute import SimpleImputer\n","\n","# Препроцессинг данных\n","\n","# Преобразование разреженной матрицы в плотную (если данные в разреженном формате)\n","# Это необходимо для корректной работы некоторых моделей и методов обработки данных.\n","X_train_class_dense = X_train_class.toarray()\n","X_test_class_dense = X_test_class.toarray()\n","\n","# Масштабирование данных\n","# StandardScaler стандартизирует данные, приводя их к нулевому среднему и единичному стандартному отклонению.\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train_class_dense)  # Обучаем scaler и применяем его к обучающим данным\n","X_test_scaled = scaler.transform(X_test_class_dense)       # Применяем обученный scaler к тестовым данным\n","\n","# Замещение пропусков (если в данных есть пропущенные значения)\n","# Используем стратегию замещения средними значениями по каждому столбцу.\n","imputer = SimpleImputer(strategy='mean')\n","X_train_imputed = imputer.fit_transform(X_train_class)  # Замещаем пропуски в обучающей выборке\n","X_test_imputed = imputer.transform(X_test_class)        # Замещаем пропуски в тестовой выборке\n","\n","# Масштабирование данных (в случае разреженных матриц)\n","# StandardScaler применяется без центрирования (with_mean=False) для совместимости с разреженными матрицами.\n","scaler = StandardScaler(with_mean=False)\n","X_train_scaled = scaler.fit_transform(X_train_imputed)  # Масштабирование обучающих данных\n","X_test_scaled = scaler.transform(X_test_imputed)        # Масштабирование тестовых данных\n","\n","# Подбор гиперпараметров через GridSearchCV\n","# Уменьшаем диапазон гиперпараметров для ускорения выполнения\n","param_grid = {\n","    'n_estimators': [100, 150],       # Количество деревьев в лесу\n","    'max_depth': [10, 15],            # Максимальная глубина дерева\n","    'min_samples_split': [2, 4]       # Минимальное количество объектов для разделения узла\n","}\n","\n","# Инициализация модели случайного леса\n","# random_state=42 фиксирует случайное состояние для воспроизводимости.\n","# n_jobs=-1 позволяет использовать все доступные ядра процессора.\n","rf_classifier = RandomForestClassifier(random_state=42, n_jobs=-1)\n","\n","# Инициализация GridSearchCV\n","# cv=3: трехкратная кросс-валидация для проверки качества на обучающей выборке.\n","# n_jobs=-1: использование всех ядер процессора для ускорения выполнения.\n","grid_search_rf = GridSearchCV(rf_classifier, param_grid, cv=3, n_jobs=-1)\n","\n","# Обучение модели с подбором гиперпараметров\n","# Обучаем GridSearchCV на масштабированных данных и метках классов\n","grid_search_rf.fit(X_train_scaled, y_train_class)\n","\n","# Вывод лучших гиперпараметров\n","print(\"Best parameters for Random Forest:\", grid_search_rf.best_params_)\n","\n","# Оценка качества модели на тестовой выборке\n","# Предсказание классов на тестовых данных с использованием лучшей модели (best_estimator_).\n","y_pred_class_rf = grid_search_rf.best_estimator_.predict(X_test_scaled)\n","\n","# Вычисление точности (accuracy)\n","# Сравниваем предсказанные метки (y_pred_class_rf) с реальными метками (y_test_class).\n","accuracy = accuracy_score(y_test_class, y_pred_class_rf)\n","\n","# Вывод точности модели\n","print(\"Accuracy of the model:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9K7nCmfWwBaI","outputId":"ed9e984e-f6cd-4990-d64a-49fba9b04559"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Best parameters for Random Forest: {'max_depth': 15, 'min_samples_split': 4, 'n_estimators': 100}\n","Accuracy of the model: 0.6926994906621392\n"]}]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import RandomizedSearchCV, train_test_split\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import numpy as np\n","\n","# Препроцессинг данных\n","# Замещение пропущенных значений средними значениями по каждому столбцу.\n","imputer = SimpleImputer(strategy=\"mean\")\n","X_train_imputed = imputer.fit_transform(X_train_reg)  # Замещаем пропуски в обучающих данных\n","X_test_imputed = imputer.transform(X_test_reg)        # Замещаем пропуски в тестовых данных\n","\n","# Подбор гиперпараметров\n","# Определяем диапазон значений для гиперпараметров:\n","# - max_depth: максимальная глубина дерева (чем больше, тем сложнее дерево).\n","# - min_samples_split: минимальное количество объектов для разделения узла.\n","# - min_samples_leaf: минимальное количество объектов в листе.\n","# - max_features: количество признаков для поиска оптимального разделения.\n","param_dist = {\n","    \"max_depth\": [3, 5, 10, 20, None],  # Ограничение глубины дерева\n","    \"min_samples_split\": [2, 5, 10],    # Минимальное число объектов для разделения узла\n","    \"min_samples_leaf\": [1, 2, 4],      # Минимальное число объектов в листе\n","    \"max_features\": [\"sqrt\", \"log2\", None]  # Число признаков для поиска разделения\n","}\n","\n","# Создание модели решающего дерева для задачи регрессии\n","# random_state=42 фиксирует случайное состояние для воспроизводимости.\n","dt_regressor = DecisionTreeRegressor(random_state=42)\n","\n","# Инициализация RandomizedSearchCV\n","# random_search выполняет случайный поиск лучших параметров из заданного распределения.\n","# - param_distributions: диапазон гиперпараметров.\n","# - n_iter=10: количество случайных комбинаций для проверки.\n","# - cv=3: трехкратная кросс-валидация для проверки качества.\n","# - n_jobs=-1: использование всех доступных ядер процессора.\n","# - error_score=\"raise\": прерывание выполнения при возникновении ошибок.\n","random_search = RandomizedSearchCV(\n","    dt_regressor,\n","    param_distributions=param_dist,\n","    n_iter=10,\n","    cv=3,\n","    random_state=42,\n","    n_jobs=-1,\n","    error_score=\"raise\"\n",")\n","\n","# Обучение модели с подбором гиперпараметров\n","# Обучаем RandomizedSearchCV на данных с замещёнными пропущенными значениями.\n","random_search.fit(X_train_imputed, y_train_reg)\n","\n","# Вывод лучших гиперпараметров\n","# Используем best_params_ для вывода параметров, которые обеспечивают наилучшее качество.\n","print(\"Лучшие параметры решающего дерева для регрессии:\", random_search.best_params_)\n","\n","# Оценка модели\n","# Получаем модель с лучшими параметрами\n","best_dt_regressor = random_search.best_estimator_\n","\n","# Предсказание значений для тестовой выборки\n","y_pred = best_dt_regressor.predict(X_test_imputed)\n","\n","# MAE (Mean Absolute Error): средняя абсолютная ошибка.\n","mae = mean_absolute_error(y_test_reg, y_pred)\n","\n","# MSE (Mean Squared Error): среднеквадратичная ошибка.\n","mse = mean_squared_error(y_test_reg, y_pred)\n","\n","# R² (R-squared): коэффициент детерминации (от 0 до 1, где 1 - идеально).\n","r2 = r2_score(y_test_reg, y_pred)\n","\n","# Вывод метрик качества модели\n","print(f\"MAE: {mae}\")  # Средняя абсолютная ошибка\n","print(f\"MSE: {mse}\")  # Среднеквадратичная ошибка\n","print(f\"R^2: {r2}\")   # Коэффициент детерминации"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvobLS-eywim","outputId":"461aa5a1-c9f0-40cc-8c51-0481b9a245c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Лучшие параметры решающего дерева для регрессии: {'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': None}\n","MAE: 268841.920963045\n","MSE: 961635486634.9545\n","R^2: 0.9837790886889036\n"]}]},{"cell_type":"markdown","source":["Решающее дерево"],"metadata":{"id":"Uya3nwxyJQhF"}},{"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","from sklearn.metrics import accuracy_score\n","from scipy.sparse import csr_matrix\n","\n","class DecisionTreeClassifierCustom:\n","    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n","        \"\"\"\n","        Инициализация пользовательского классификатора решающего дерева.\n","\n","        Параметры:\n","        - max_depth: максимальная глубина дерева (по умолчанию None - без ограничения).\n","        - min_samples_split: минимальное количество объектов для разделения узла.\n","        - min_samples_leaf: минимальное количество объектов в листе.\n","        \"\"\"\n","        self.max_depth = max_depth\n","        self.min_samples_split = min_samples_split\n","        self.min_samples_leaf = min_samples_leaf\n","        self.tree = None  # Структура дерева будет сохранена после обучения\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Обучение модели на тренировочных данных.\n","\n","        Параметры:\n","        - X: матрица признаков (возможно, разреженная).\n","        - y: вектор меток классов.\n","        \"\"\"\n","        # Преобразуем разреженную матрицу в плотную для удобства работы\n","        if isinstance(X, csr_matrix):\n","            X = X.toarray()\n","        # Строим дерево рекурсивно\n","        self.tree = self._build_tree(X, y)\n","\n","    def _build_tree(self, X, y, depth=0):\n","        \"\"\"\n","        Рекурсивное построение дерева.\n","\n","        Параметры:\n","        - X: матрица признаков.\n","        - y: метки классов.\n","        - depth: текущая глубина дерева.\n","\n","        Возвращает:\n","        - Лист или узел дерева.\n","        \"\"\"\n","        n_samples, n_features = X.shape\n","        unique_classes = np.unique(y)\n","\n","        # Условия остановки\n","        if len(unique_classes) == 1 or (self.max_depth and depth >= self.max_depth):\n","            return Counter(y).most_common(1)[0][0]  # Возвращаем наиболее частый класс\n","\n","        # Ищем лучший раздел\n","        best_split = self._best_split(X, y)\n","        if best_split is None:\n","            return Counter(y).most_common(1)[0][0]  # Возвращаем наиболее частый класс, если раздел невозможен\n","\n","        # Рекурсивно строим левую и правую части дерева\n","        left_tree = self._build_tree(X[best_split['left_indices']], y[best_split['left_indices']], depth + 1)\n","        right_tree = self._build_tree(X[best_split['right_indices']], y[best_split['right_indices']], depth + 1)\n","\n","        # Возвращаем текущий узел дерева\n","        return {\n","            'feature_index': best_split['feature_index'],\n","            'threshold': best_split['threshold'],\n","            'left': left_tree,\n","            'right': right_tree\n","        }\n","\n","    def _best_split(self, X, y):\n","        \"\"\"\n","        Поиск наилучшего раздела по критерию Джини.\n","\n","        Параметры:\n","        - X: матрица признаков.\n","        - y: метки классов.\n","\n","        Возвращает:\n","        - Информацию о лучшем разрезе или None, если разрез невозможен.\n","        \"\"\"\n","        best_gini = float(\"inf\")\n","        best_split = {}\n","        n_samples, n_features = X.shape\n","\n","        for feature_index in range(n_features):\n","            thresholds = np.unique(X[:, feature_index])  # Возможные значения разделов\n","            for threshold in thresholds:\n","                left_indices = X[:, feature_index] <= threshold\n","                right_indices = X[:, feature_index] > threshold\n","\n","                # Пропускаем маленькие разделы\n","                if np.sum(left_indices) < self.min_samples_leaf or np.sum(right_indices) < self.min_samples_leaf:\n","                    continue\n","\n","                gini = self._gini_index(y[left_indices], y[right_indices])\n","\n","                if gini < best_gini:\n","                    best_gini = gini\n","                    best_split = {\n","                        'feature_index': feature_index,\n","                        'threshold': threshold,\n","                        'left_indices': left_indices,\n","                        'right_indices': right_indices\n","                    }\n","\n","        return best_split if best_gini != float(\"inf\") else None\n","\n","    def _gini_index(self, left_y, right_y):\n","        \"\"\"\n","        Вычисление индекса Джини для раздела.\n","\n","        Параметры:\n","        - left_y: метки классов для левой части.\n","        - right_y: метки классов для правой части.\n","\n","        Возвращает:\n","        - Значение индекса Джини.\n","        \"\"\"\n","        left_size = len(left_y)\n","        right_size = len(right_y)\n","        total_size = left_size + right_size\n","\n","        left_prob = np.array([np.sum(left_y == c) / left_size for c in np.unique(left_y)])\n","        right_prob = np.array([np.sum(right_y == c) / right_size for c in np.unique(right_y)])\n","\n","        gini_left = 1 - np.sum(left_prob ** 2)\n","        gini_right = 1 - np.sum(right_prob ** 2)\n","\n","        return (left_size / total_size) * gini_left + (right_size / total_size) * gini_right\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Предсказание меток классов для новых данных.\n","\n","        Параметры:\n","        - X: матрица признаков.\n","\n","        Возвращает:\n","        - Вектор предсказанных меток классов.\n","        \"\"\"\n","        if isinstance(X, csr_matrix):\n","            X = X.toarray()  # Преобразуем разреженную матрицу в плотную\n","        return np.array([self._predict_sample(x, self.tree) for x in X])\n","\n","    def _predict_sample(self, x, tree):\n","        \"\"\"\n","        Предсказание для одного объекта.\n","\n","        Параметры:\n","        - x: вектор признаков.\n","        - tree: текущий узел дерева.\n","\n","        Возвращает:\n","        - Предсказанная метка класса.\n","        \"\"\"\n","        if not isinstance(tree, dict):  # Если текущий узел — лист, возвращаем класс\n","            return tree\n","\n","        if x[tree['feature_index']] <= tree['threshold']:  # Сравнение с порогом\n","            return self._predict_sample(x, tree['left'])\n","        else:\n","            return self._predict_sample(x, tree['right'])"],"metadata":{"id":"2tgUTf6FCg0S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Создание экземпляра пользовательского классификатора решающего дерева\n","# max_depth=5: ограничиваем максимальную глубину дерева до 5.\n","# min_samples_split=10: минимальное количество объектов для разделения узла.\n","# min_samples_leaf=5: минимальное количество объектов в листе.\n","classifier = DecisionTreeClassifierCustom(max_depth=5, min_samples_split=10, min_samples_leaf=5)\n","\n","# Обучение модели на тренировочных данных\n","# X_train_class: матрица признаков для обучения.\n","# y_train_class: метки классов для обучения.\n","classifier.fit(X_train_class, y_train_class)\n","\n","# Прогнозирование классов для тестовых данных\n","# X_test_class: матрица признаков для тестовой выборки.\n","y_pred = classifier.predict(X_test_class)\n","\n","# Оценка качества модели\n","# Вычисление точности (accuracy) предсказаний.\n","# Сравниваем предсказанные метки (y_pred) с реальными метками (y_test_class).\n","accuracy = accuracy_score(y_test_class, y_pred)\n","\n","# Вывод точности модели\n","print(f\"Accuracy: {accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlKTmQd4CiZc","outputId":"e6d75740-3355-4cc0-c9b1-0b4763511260"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.7923033389926429\n"]}]},{"cell_type":"markdown","source":["# Кастомная регрессия"],"metadata":{"id":"kvpZyoj2JXsN"}},{"cell_type":"code","source":["class DecisionTreeRegressorCustom:\n","    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n","        \"\"\"\n","        Инициализация пользовательского регрессора решающего дерева.\n","\n","        Параметры:\n","        - max_depth: максимальная глубина дерева (по умолчанию None - без ограничения).\n","        - min_samples_split: минимальное количество объектов для разделения узла.\n","        - min_samples_leaf: минимальное количество объектов в листе.\n","        \"\"\"\n","        self.max_depth = max_depth  # Максимальная глубина дерева\n","        self.min_samples_split = min_samples_split  # Минимальное число объектов для разделения узла\n","        self.min_samples_leaf = min_samples_leaf  # Минимальное число объектов в листе\n","        self.tree = None  # Структура дерева будет сохранена после обучения\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Обучение модели на тренировочных данных.\n","\n","        Параметры:\n","        - X: матрица признаков.\n","        - y: целевая переменная.\n","        \"\"\"\n","        self.tree = self._build_tree(X, y)\n","\n","    def _build_tree(self, X, y, depth=0):\n","        \"\"\"\n","        Рекурсивное построение дерева.\n","\n","        Параметры:\n","        - X: матрица признаков.\n","        - y: целевая переменная.\n","        - depth: текущая глубина дерева.\n","\n","        Возвращает:\n","        - Узел дерева (лист или разветвление).\n","        \"\"\"\n","        n_samples, n_features = X.shape\n","        unique_values = np.unique(y)\n","\n","        # Условие остановки\n","        if len(unique_values) == 1:  # Если все значения целевой переменной одинаковы\n","            return {'value': unique_values[0]}\n","        if depth >= self.max_depth or n_samples < self.min_samples_split:  # Проверяем ограничение глубины или объектов\n","            return {'value': np.mean(y)}  # Возвращаем среднее значение как предсказание\n","\n","        # Поиск лучшего разбиения\n","        best_split = None\n","        best_score = float('inf')\n","        for feature_index in range(n_features):  # Проходим по всем признакам\n","            thresholds = np.unique(X[:, feature_index])  # Уникальные значения признака\n","            for threshold in thresholds:  # Перебираем все возможные пороги\n","                left_indices = X[:, feature_index] <= threshold  # Индексы объектов, которые идут влево\n","                right_indices = ~left_indices  # Индексы объектов, которые идут вправо\n","                left_y = y[left_indices]  # Левая часть целевой переменной\n","                right_y = y[right_indices]  # Правая часть целевой переменной\n","\n","                # Проверяем минимальное количество объектов в листьях\n","                if len(left_y) >= self.min_samples_leaf and len(right_y) >= self.min_samples_leaf:\n","                    gini = self._calculate_gini(left_y, right_y)  # Вычисляем критерий Джини\n","                    if gini < best_score:  # Сохраняем лучшее разбиение\n","                        best_score = gini\n","                        best_split = (feature_index, threshold)\n","\n","        if best_split is None:  # Если не найдено подходящее разбиение\n","            return {'value': np.mean(y)}  # Возвращаем среднее значение\n","\n","        # Рекурсивно строим дерево\n","        feature_index, threshold = best_split\n","        left_indices = X[:, feature_index] <= threshold\n","        right_indices = ~left_indices\n","        left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n","        right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n","\n","        # Возвращаем текущий узел дерева\n","        return {'feature_index': feature_index, 'threshold': threshold, 'left': left_tree, 'right': right_tree}\n","\n","    def _calculate_gini(self, left_y, right_y):\n","        \"\"\"\n","        Вычисление критерия Джини для раздела.\n","\n","        Параметры:\n","        - left_y: левая часть целевой переменной.\n","        - right_y: правая часть целевой переменной.\n","\n","        Возвращает:\n","        - Значение критерия Джини.\n","        \"\"\"\n","        left_size = len(left_y)\n","        right_size = len(right_y)\n","        total_size = left_size + right_size\n","        left_gini = 1 - sum((np.sum(left_y == label) / left_size) ** 2 for label in np.unique(left_y))\n","        right_gini = 1 - sum((np.sum(right_y == label) / right_size) ** 2 for label in np.unique(right_y))\n","        return (left_size / total_size) * left_gini + (right_size / total_size) * right_gini\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Предсказание значений целевой переменной для новых данных.\n","\n","        Параметры:\n","        - X: матрица признаков.\n","\n","        Возвращает:\n","        - Вектор предсказанных значений.\n","        \"\"\"\n","        predictions = [self._predict_sample(sample, self.tree) for sample in X]\n","        return np.array(predictions)\n","\n","    def _predict_sample(self, sample, tree):\n","        \"\"\"\n","        Предсказание для одного объекта.\n","\n","        Параметры:\n","        - sample: вектор признаков объекта.\n","        - tree: текущий узел дерева.\n","\n","        Возвращает:\n","        - Предсказанное значение целевой переменной.\n","        \"\"\"\n","        if 'value' in tree:  # Если это лист\n","            return tree['value']\n","\n","        feature_value = sample[tree['feature_index']]  # Значение признака\n","        if feature_value <= tree['threshold']:  # Сравнение с порогом\n","            return self._predict_sample(sample, tree['left'])  # Рекурсия в левое поддерево\n","        else:\n","            return self._predict_sample(sample, tree['right'])  # Рекурсия в правое поддерево"],"metadata":{"id":"LDRJtYiOJdfb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Создание экземпляра пользовательского регрессора решающего дерева\n","# max_depth=5: ограничиваем максимальную глубину дерева до 5.\n","# min_samples_split=10: минимальное количество объектов для разделения узла.\n","# min_samples_leaf=5: минимальное количество объектов в каждом листе.\n","regressor = DecisionTreeRegressorCustom(max_depth=5, min_samples_split=10, min_samples_leaf=5)\n","\n","# Обучение модели на тренировочных данных\n","# Используем .values для преобразования данных в массивы NumPy, так как пользовательская модель\n","# ожидает входные данные в формате NumPy.\n","regressor.fit(X_train_reg.values, y_train_reg.values)\n","\n","# Прогнозирование значений целевой переменной на тестовых данных\n","# X_test_reg.values: матрица признаков тестовой выборки в формате NumPy.\n","y_pred = regressor.predict(X_test_reg.values)\n","\n","# Оценка качества модели\n","\n","# MAE (Mean Absolute Error): средняя абсолютная ошибка.\n","# Показывает среднее абсолютное отклонение предсказанных значений от реальных.\n","mae = mean_absolute_error(y_test_reg, y_pred)\n","\n","# MSE (Mean Squared Error): среднеквадратичная ошибка.\n","# Указывает на средний квадрат разницы между предсказанными и реальными значениями.\n","mse = mean_squared_error(y_test_reg, y_pred)\n","\n","# Вывод метрик качества модели\n","print(f\"MAE: {mae}\")  # Средняя абсолютная ошибка\n","print(f\"MSE: {mse}\")  # Среднеквадратичная ошибка"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gr4uNqEDJ66j","outputId":"79293ad6-6539-4dde-db74-b55a688b43fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-54-47e252df4b0b>:28: RuntimeWarning: invalid value encountered in less_equal\n","  left_indices = X[:, feature_index] <= threshold\n"]},{"output_type":"stream","name":"stdout","text":["MAE: 1258357.4966011762\n","MSE: 11770383511829.137\n"]}]},{"cell_type":"markdown","source":["# Решающие деревья показали свою эффективность как для классификации, так и для регрессии. Основные выводы:\n","\t•\tРешающие деревья хорошо подходят для работы с разнородными данными.\n","\t•\tГрамотная настройка гиперпараметров позволяет значительно улучшить результаты.\n","\t•\tПользовательские реализации дают возможность глубже понять алгоритм, но для реальных задач целесообразнее использовать библиотечные решения."],"metadata":{"id":"arti3kUQzgtk"}}]}